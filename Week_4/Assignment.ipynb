{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convolutional Neural Networks (CNNS) for Image Classification**\n",
    "\n",
    "In this assignment, we explore the capabilities of a popular deep learning architectures — Convolutional Neural Networks (CNNs) — by training them on various datasets and comparing their performance. While ANNs serve as a versatile model for a range of tasks, CNNs are specifically designed for handling spatial data, making them particularly effective for image classification problems. By evaluating these models, we aim to highlight their respective strengths, limitations, and suitability for different types of data, providing insights into their real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 1**\n",
    "This block imports essential libraries needed for building and training a Convolutional Neural Network (CNN) with PyTorch, including data loading, transformations, and metrics for evaluation.\n",
    "1) <blue>**torch**</blue>: The core PyTorch library, essential for all operations involving tensors, model creation, and training.\n",
    "2) <blue>**torch.nn**</blue>: Contains modules to define neural network architectures.\n",
    "3) <blue>**torch.optim**</blue>: Provides optimization algorithms like <green>**SGD (Stochastic Gradient Descent)**</green>, used during model training.\n",
    "4) <blue>**torchvision.transforms**</blue>: A module for applying various image transformations, such as <green>**normalization**</green> or <green>**random cropping**</green>.\n",
    "5) <blue>**DataLoader**</blue>: Used to efficiently load data in batches, critical for training deep learning models.\n",
    "6) <blue>**sklearn.metrics**</blue>: Includes metrics to evaluate model performance, such as <green>**accuracy**</green> and <green>**precision**</green>.\n",
    "7) <blue>**seaborn/matplotlib**</blue>: Libraries for <green>**visualization**</green>, typically used to visualize model performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 2**\n",
    "This block configures the device (CPU or GPU) and applies data transformations to the images (conversion to tensors and normalization).\n",
    "1) <blue>**torch.device**</blue>: Chooses the <green>**device**</green> to run computations on. If a <green>**GPU**</green> is available, the model will run on it; otherwise, it defaults to the CPU.\n",
    "2) <blue>**transforms.Compose**</blue>: Combines multiple transformations to apply <green>**sequentially**</green> to the data.\n",
    "3) <blue>**transforms.ToTensor()**</blue>: Converts the image from a <green>**PIL Image**</green> (or numpy array) to a PyTorch tensor.\n",
    "4) <blue>**transforms.Normalize()**</blue>: Normalizes the pixel values of the image. Each <green>**channel (R, G, B)**</green> is normalized with a <blue>**mean**</blue> of <green>**0.5**</green> and a <blue>**standard deviation**</blue> of <green>**0.5**</green>, scaling the values to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 3**\n",
    "This block is meant to load the CIFAR-10 dataset, though the actual dataset loading is marked as a TODO. It sets up data loaders for both the training and testing datasets.\n",
    "1) <blue>**train_dataset/test_dataset**</blue>: These will eventually contain the training and testing datasets. The <blue>**torchvision.datasets.CIFAR10**</blue> class will likely be used here.\n",
    "2) <blue>**train_loader**</blue>: Loads the training dataset in <blue>**batches**</blue> of <green>**64**</green> images and <green>**shuffles**</green> them to ensure randomness during training.\n",
    "3) <blue>**test_loader**</blue>: Loads the test dataset in <blue>**batches**</blue> of <green>**64**</green> images but <green>**does not shuffle**</green> them since testing doesn't require randomization.\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate code to download the training and testing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 4** \n",
    "1) This code defines a simple <green>**feedforward artificial neural network (ANN)**</green> for classification.\n",
    "2) The model has three fully connected layers <blue>**(fc1, fc2, fc3)**</blue>, and the final layer outputs predictions for <green>**10 classes** </green>.\n",
    "3) The <green>**forward pass**</green> describes how the input data flows through the network\n",
    "4) The input image is first <green>**flattened**</green>.\n",
    "5) It passes through fully connected layers with <blue>**ReLU activation function**</blue>.\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate numbers of units and activation functions in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        # Define the layers for the ANN (Flatten, Fully Connected, Activation functions)\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)    # Input layer\n",
    "        self.fc2 = nn.Linear(512, 256)        # Hidden layer\n",
    "        self.fc3 = nn.Linear(256, 10)         # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)               # Flatten the input image\n",
    "        x = torch.relu(self.fc1(x))           # First fully connected layer + relu activation\n",
    "        x = torch.relu(self.fc2(x))           # Second fully connected layer + relu activation\n",
    "        x = self.fc3(x)                       # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 5**\n",
    "1) The <blue>**ANN model**</blue> is instantiated and moved to the <green>**selected device (CPU or GPU)**</green>.\n",
    "2) The loss function is set to <blue>**CrossEntropyLoss**</blue>, which is suitable for <green>**multi-class classification**</green> problems like in our case of CIFAR-10.\n",
    "3) The optimizer is <blue>**Adam**</blue>, with a <blue>**learning rate**</blue> of <green>**0.001**</green>, used to adjust the model parameters during training based on gradients computed from the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN model, loss function, and optimizer\n",
    "model_ann = ANN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ann.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 6**\n",
    "This code trains the neural network over a specified number of epochs (num_epochs).\n",
    "For each batch of images and labels, the following steps are performed:\n",
    "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU or GPU)**</green>.\n",
    "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute the <green>**output predictions**</green>.\n",
    "3) <blue>**Loss Calculation**</blue>: The loss between the predictions and true labels is computed and added to <blue>**ls_losses**</blue> for tracking.\n",
    "4) <blue>**Backpropagation and Optimization**</blue>: Gradients are calculated using backpropagation, and the optimizer <green>**updates the model parameters**</green> based on these gradients.\n",
    "\n",
    "Every 100 batches, the loss is printed to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ANN model\n",
    "num_epochs = 5\n",
    "ls_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_ann(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        ls_losses.append(loss.item())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 7**\n",
    "1) This code plots the <blue>**training losses**</blue> that were recorded in <blue>**ls_losses**</blue> during training.\n",
    "2) The <blue>**x-axis**</blue> represents the <green>**number of samples (batches)**</green> seen during training, and the <blue>**y-axis**</blue> shows the <green>**corresponding loss**</green>.\n",
    "3) It provides a visual representation of how the <green>**model's loss decreases over time**</green>, indicating whether the training is progressing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Losses\n",
    "x_axis = np.arange(0, len(ls_losses), 1)\n",
    "plt.plot(x_axis, ls_losses)\n",
    "plt.xlabel = \"Sample\"\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 8**\n",
    "This block evaluates the model on the test dataset.\\\n",
    "<blue>**model.eval()**</blue> sets the model to <green>**evaluation mode**</green>, disabling dropout layers and stopping the computation of gradients to save memory and speed up computations.\n",
    "For each batch of test images:\n",
    "1) <blue>**Data Movement**</blue>: Images and labels are moved to the <green>**device (CPU/GPU)**</green>.\n",
    "2) <blue>**Forward Pass**</blue>: Images pass through the network to compute <green>**predictions**</green>.\n",
    "3) <blue>**Storing Results**</blue>: Predictions and true labels are saved to <green>**calculate metrics**</green> later.\n",
    "\n",
    "This code also calculates key performance metrics to evaluate the model:\n",
    "1) <blue>**Accuracy**</blue>: Percentage of correctly classified samples.\n",
    "2) <blue>**Precision**</blue>: Proportion of true positive predictions out of all positive predictions.\n",
    "3) <blue>**Recall**</blue>: Proportion of true positives out of actual positive samples.\n",
    "4) <blue>**F1-Score**</blue>: <green>**Harmonic mean**</green> of precision and recall.\n",
    "5) The <blue>**confusion matrix**</blue> is also calculated, showing the number of correct and incorrect predictions for each class. It is visualized using a <green>**heatmap**</green>, where the <blue>**rows**</blue> represent <green>**true labels**</green>, and the <blue>**columns**</blue> represent <green>**predicted labels**</green>.\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model_ann.eval()\n",
    "all_preds_ann = []\n",
    "all_labels_ann = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass \n",
    "        outputs = model_ann(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds_ann.extend(predicted.cpu().numpy())\n",
    "        all_labels_ann.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score using sklearn\n",
    "accuracy_ann = accuracy_score(all_labels_ann, all_preds_ann)\n",
    "precision_ann = precision_score(all_labels_ann, all_preds_ann, average='weighted')\n",
    "recall_ann = recall_score(all_labels_ann, all_preds_ann, average='weighted')\n",
    "f1_ann = f1_score(all_labels_ann, all_preds_ann, average='weighted')\n",
    "\n",
    "print(f\"ANN Accuracy: {accuracy_ann:.4f}\")\n",
    "print(f\"ANN Precision: {precision_ann:.4f}\")\n",
    "print(f\"ANN Recall: {recall_ann:.4f}\")\n",
    "print(f\"ANN F1-Score: {f1_ann:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels_ann, all_preds_ann)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - ANN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 9**\n",
    "This block defines a simple Convolutional Neural Network (CNN) architecture using PyTorch. The model will consist of convolutional layers, activation functions, pooling layers, and fully connected layers to classify images from the CIFAR-10 dataset.\n",
    "1) <blue>**class CNN(nn.Module)**</blue>: This defines a custom CNN class that inherits from PyTorch's nn.Module, the base class for all neural networks in PyTorch.\n",
    "2) <blue>**self.conv**</blue>: These define the two convolutional layers. The first takes an <blue>**input**</blue> with <green>**3 channels (RGB)**</green> and produces <green>**32**</green> <blue>**feature maps**</blue>. The second takes <green>**32**</green> <blue>**input channels**</blue> and produces <green>**64**</green> <blue>**feature maps**</blue>.\n",
    "3) <blue>**self.pool**</blue>: A max-pooling layer that reduces the size of the feature maps by <green>**half (downsampling)**</green>. It takes the maximum value over a <green>**2x2**</green> <blue>**grid**</blue> with a <blue>**stride**</blue> of <green>**2**</green>.\n",
    "4) <blue>**self.fc**</blue>: Fully connected layers. The first layer takes the flattened feature maps from the convolutional layers as input and <blue>**outputs**</blue> <green>**512**</green> features. The second layer maps the <green>**512**</green> <blue>**features**</blue> to <green>**10**</green> <blue>**output classes**</blue> (for the 10 CIFAR-10 categories).\n",
    "5) <blue>**forward(self, x)**</blue>: This function defines how data flows through the network. It applies the convolutional layers, pooling, and fully connected layers in sequence.\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define the CNN layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # 3 input channels, 32 output channels\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 32 input channels, 64 output channels\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)                     # Flattened size after 2x pooling\n",
    "        self.fc2 = nn.Linear(512, 10)                             # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))                  # First conv layer + relu + pool\n",
    "        x = self.pool(torch.relu(self.conv2(x)))                  # Second conv layer + relu + pool\n",
    "        x = x.view(-1, 64 * 8 * 8)                                # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))                               # Fully connected layer + relu\n",
    "        x = self.fc2(x)                                           # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 10**\n",
    "This block defines the training loop for the CNN model. It will train the model over multiple epochs, compute the loss using cross-entropy, and optimize the model using an optimizer like SGD.\n",
    "1) <blue>**optimizer.zero_grad()**</blue>: <green>**Clears**</green> the <blue>**gradients**</blue> from the previous iteration to prevent accumulation.\n",
    "2) <blue>**loss.backward()**</blue>: Computes the <blue>**gradients**</blue> via <green>**backpropagation**</green>.\n",
    "3) <blue>**optimizer.step()**</blue>: Updates the model's parameters based on the <green>**computed gradients**</green>.\n",
    "4) <blue>**running_loss**</blue>: Keeps track of the <blue>**cumulative loss**</blue> for the epoch, which is divided by the <green>**number of batches**</green> to return the <blue>**average loss**</blue>.\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for CNN\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 11**\n",
    "This code performs evaluation on the test set by moving data to the appropriate device, running the model to get predictions, and then calculating key performance metrics using the predicted and actual labels.\n",
    "1) <blue>**model_cnn.eval()**</blue>: The code begins by setting the model to <green>**evaluation mode**</green> to ensure proper inference behavior.\n",
    "2) <blue>**all_preds_cnn, all_labels_cnn**</blue>: These lists are initialized to store <green>**predicted**</green> and <green>**true labels**</green>, respectively.\n",
    "3) <blue>**torch.no_grad()**</blue>: This is used to <green>**disable gradients**</green>, save memory and speed up computations, the test set is processed in batches from test_loader, where both images and labels are moved to the correct device.\n",
    "6) After processing the test set, evaluation metrics such as <blue>**accuracy**</blue>, <blue>**precision**</blue>, <blue>**recall**</blue>, and <blue>**F1-score**</blue> are calculated.\n",
    "7) Finally, the results are printed to display the performance of the CNN model\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the CNN model\n",
    "model_cnn.eval()\n",
    "all_preds_cnn = []\n",
    "all_labels_cnn = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Move images and labels to the device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model_cnn(images)  # Forward pass through the CNN\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds_cnn.extend(predicted.cpu().numpy())\n",
    "        all_labels_cnn.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score for the CNN\n",
    "accuracy_cnn = accuracy_score(all_labels_cnn, all_preds_cnn)\n",
    "precision_cnn = precision_score(all_labels_cnn, all_preds_cnn, average='weighted')\n",
    "recall_cnn = recall_score(all_labels_cnn, all_preds_cnn, average='weighted')\n",
    "f1_cnn = f1_score(all_labels_cnn, all_preds_cnn, average='weighted')\n",
    "\n",
    "print(f\"CNN Accuracy: {accuracy_cnn:.4f}\")\n",
    "print(f\"CNN Precision: {precision_cnn:.4f}\")\n",
    "print(f\"CNN Recall: {recall_cnn:.4f}\")\n",
    "print(f\"CNN F1-Score: {f1_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "### **Step - 12**\n",
    "This code block compares performance of the trained ANN and CNN models, showing which one performs better.\n",
    "1) <blue>**import pandas as pd**</blue>: The code starts by importing the <green>**Pandas library**</green>, which is used for data manipulation and creation of a comparison table.\n",
    "2) <blue>**data dictionary**</blue>: A dictionary is defined with the keys 'Model', 'Accuracy', 'Precision', 'Recall', and 'F1-Score', containing <green>**placeholders (None)**</green> for both the 'ANN' and 'CNN' models. These placeholders will later hold the actual performance metrics.\n",
    "3) <blue>**pd.DataFrame(data)**</blue>: The dictionary is converted into a <green>**Pandas DataFrame**</green>, which provides a tabular structure for easy comparison of the metrics between the two models.\n",
    "4) <blue>**print(df_comparison)**</blue>: This line prints the <green>**comparison table**</green> of ANN and CNN metrics, enabling a visual representation of their performance.\n",
    "\n",
    "Note: Replace the placeholders **\"None\"** with the appropriate code to calculate the metrics using sklearn and compute the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the comparison table with the metrics from ANN and CNN\n",
    "data = {\n",
    "    'Model': ['ANN', 'CNN'],\n",
    "    'Accuracy': [accuracy_ann, accuracy_cnn],\n",
    "    'Precision': [precision_ann, precision_cnn],\n",
    "    'Recall': [recall_ann, recall_cnn],\n",
    "    'F1-Score': [f1_ann, f1_cnn]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(data)\n",
    "print(df_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
