{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment**\n",
    "## **Fine-tuning a Pretrained ResNet model for Image Classification**\n",
    "\n",
    "In this assignment, we will explore the concept of transfer learning by fine-tuning a pretrained ResNet model for the task of image classification. ResNet, or Residual Network, is a deep convolutional neural network architecture that has achieved state-of-the-art performance on a variety of visual recognition tasks. Pretrained models are models that have been previously trained on large datasets, such as ImageNet, and can serve as a starting point for new tasks by leveraging learned features. By fine-tuning the pretrained ResNet, we aim to adapt its feature representations to a new dataset, improving both training efficiency and model performance. This process involves adjusting the modelâ€™s final layers to suit the specific classification problem, while retaining the learned features from earlier layers. At the end we compare the prformance of the pre-trained model with that of the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 1**\n",
    "1) <blue>**torch**</blue>: This imports the <green>**PyTorch library**</green>, which is essential for building and training neural networks.\n",
    "2) <blue>**torch.nn**</blue>: Imports the <green>**neural network module**</green> of PyTorch, which contains various layers and loss functions used to build models.\n",
    "3) <blue>**torch.optim**</blue>: This imports the <green>**optimization algorithms**</green> module, used for updating the model's weights based on the computed gradients during training.\n",
    "4) <blue>**torchvision -> datasets, models, transforms**</blue>: Imports functions from <green>**Torchvision**</green> for accessing popular datasets, pre-trained models, and transformations for image preprocessing.\n",
    "5) <blue>**torchvision.utils -> make_grid**</blue>: This function is used to <green>**visualize multiple images**</green> in a grid format, useful for displaying image batches.\n",
    "6) <blue>**torch.utils.data -> DataLoader**</blue>: Imports the <green>**DataLoader**</green> class, which allows for efficient loading and batching of datasets during training and testing.\n",
    "7) <blue>**matplotlib.pyplot**</blue>: This imports <green>**Matplotlib**</green>, which is used for plotting and visualizing images or training metrics such as loss and accuracy.\n",
    "8) <blue>**numpy**</blue>: Imports <green>**NumPy**</green> for handling arrays and performing numerical operations, which are often useful in preprocessing and analyzing data.\n",
    "9) <blue>**time**</blue>: This allows tracking <green>**elapsed time**</green> during the training process, often used to measure performance.\n",
    "10) <blue>**os**</blue>: Imports <green>**OS module**</green> to handle tasks like creating directories, managing file paths, or interacting with the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 2**\n",
    "1) <blue>**device**</blue>: This variable checks if a <green>**CUDA-capable GPU**</green> is available; if so, it sets the device to GPU (cuda), otherwise, it falls back to the <green>**CPU**</green>.\n",
    "2) <blue>**transforms.Compose**</blue>: This creates a series of <green>**transformations for training images**</green>, applied in the specified order to augment the data and prepare it for the model.\n",
    "3) <blue>**transforms.Grayscale(num_output_channels=3)**</blue>: This transformation converts input images from grayscale to <green>**RGB**</green> by creating 3 output channels, which is necessary for models expecting RGB inputs.\n",
    "4) <blue>**transforms.RandomResizedCrop(224)**</blue>: This randomly crops the input image to a size of <green>**224x224 pixels**</green>, introducing variation in the training data to improve model generalization.\n",
    "5) <blue>**transforms.RandomHorizontalFlip()**</blue>: This randomly flips the image horizontally with a 50% chance, providing additional <green>**data augmentation**</green> for the training dataset.\n",
    "6) <blue>**transforms.ToTensor()**</blue>: Converts the PIL image or NumPy array into a <green>**PyTorch tensor**</green>, which is the required input format for models.\n",
    "7) <blue>**transforms.Normalize()**</blue>: This normalizes the tensor with mean and standard deviation values specified for each channel, helping the model to <green>**converge faster**</green> and perform better by ensuring the input data has a consistent scale.\n",
    "8) <blue>**transforms.Resize(256)**</blue>: This resizes the input image to a height and width of <green>**256 pixels**</green>, ensuring consistency in input size before cropping.\n",
    "9) <blue>**transforms.CenterCrop(224)**</blue>: This crops the center of the image to <green>**224x224 pixels**</green>, focusing on the most important part of the image for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformations for training and testing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels (RGB)\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels (RGB)\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 3**\n",
    "1) This code outlines loading the <blue>**Caltech-256**</blue> dataset for <green>**image classification**</green>.\n",
    "2) The <blue>**train_loader**</blue> and <blue>**test_loader**</blue> are used to load the datasets in batches of 32, with shuffling applied only to the training set.\n",
    "\n",
    "Note: The placeholders **\"None\"** need to be replaced with the correct dataset-loading code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Caltech-256 dataset, download and load it through torchvision.datasets.ImageFolder\n",
    "# Replace 'path/to/256_ObjectCategories' with the actual path to your Caltech-256 dataset directory\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=os.path.join('path/to/256_ObjectCategories'),  # Update this path as needed\n",
    "    transform=transform_train\n",
    ")\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=os.path.join('path/to/256_ObjectCategories'),  # Update this path as needed\n",
    "    transform=transform_test\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 4**\n",
    "1) <blue>**model = None**</blue>: This line will load a <green>**pre-trained ResNet-18 model**</green> from the Torchvision library, which has been trained on the ImageNet dataset, allowing for transfer learning, when None is replaced with proper code.\n",
    "2) <blue>**for param in model.parameters()**:</blue>: This loop iterates over all the parameters of the model, enabling modifications to their properties.\n",
    "3) <blue>**param.requires_grad = None**</blue>: Inside the loop, this line will be used to <green>**freeze the model weights**</green> for all layers except the final fully connected layer. By setting requires_grad to False, gradients will not be calculated for these parameters during backpropagation, preventing them from being updated during training, when None is replace by proper code\n",
    "4) <blue>**num_ftrs = model.fc.in_features**</blue>: This line retrieves the number of input features from the final fully connected layer of the model, which is necessary for modifying that layer.\n",
    "5) <blue>**model.fc = nn.Linear(num_ftrs, 257)**</blue>: This modifies the final fully connected layer to output <green>**257 classes**</green>, corresponding to the number of classes in the Caltech-256 dataset, replacing the original output layer.\n",
    "\n",
    "Note: The placeholders **\"None\"** need to be replaced with the correct code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet-18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze the model weights except for the final layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final fully connected layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 257)  # 257 classes in Caltech-256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 5**\n",
    "1) <blue>**model = None**</blue>: This line sends the <green>**model to the appropriate device**</green>, whether it's a GPU (if available) or a CPU, ensuring that the computations happen on the selected hardware, when None is replace with correct code.\n",
    "2) <blue>**criterion = None**</blue>: Defines the <green>**loss function**</green> as cross-entropy loss, which is commonly used for multi-class classification problems, when None is replaced with the correct code.\n",
    "3) <blue>**optimizer = None**</blue>: Initialize the <green>**Adam optimizer**</green> to update only the parameters of the modified final layer (model.fc). Set the learning rate to <green>**0.001**</green> to control the step size for each update during training.\n",
    "\n",
    "Note: The placeholders **\"None\"** need to be replaced with the correct code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 6**\n",
    "1) <blue>**train_model**</blue>: This function trains and evaluates the model for a specified number of <green>**epochs**</green>.\n",
    "2) It <green>**alternates**</green> between training and testing phases, setting the model to <blue>**train() or eval()**</blue> mode accordingly.\n",
    "3) <blue>**images = None, labels = None**</blue>: Images and labels are moved to the <green>**device (CPU or GPU)**</green>.\n",
    "4) <blue>**Backpropagation and Optimization**</blue>: Gradients are calculated using backpropagation, and the optimizer <green>**updates the model parameters**</green> based on these gradients.\n",
    "\n",
    "Note: The placeholders **\"None\"** need to be replaced with the correct code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and testing phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "                dataloader = test_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move images and labels to the device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = None\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    # Forward Pass\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) \n",
    "                    loss = None # Replace None with the correct code to find error between labels and outputs\n",
    "\n",
    "                    # Backward + optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tuned_model = train_model(model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 7**\n",
    "The provided code consists of three functions designed for model evaluation and visualization:\n",
    "1) <blue>**visualize_predictions**</blue>: This function takes a <green>**trained model**</green> and a <green>**dataloader**</green>, then displays a set number of images with their predicted labels. It sets the model to <blue>**evaluation mode**</blue> and processes the input data without <green>**calculating gradients**</green> (using <blue>**torch.no_grad()**</blue>), which speeds up the evaluation. After moving the inputs to the correct device, it computes predictions using a forward pass through the model and visualizes the results in a subplot.\n",
    "2) <blue>**denormalize**</blue>: This utility function converts a <blue>**normalized image tensor**</blue> back to its <blue>**original form**</blue> by <green>**reversing the normalization process**</green>. It applies the <green>**mean**</green> and <green>**standard deviation**</green> used during image preprocessing, ensuring the values are scaled back into the range suitable for visualization (<blue>**clipped between 0 and 1**</blue>).\n",
    "3) <blue>**make_map_classes**</blue>: This function generates a <blue>**dictionary mapping class indices**</blue> to their corresponding class names. It reads the folder names (assuming they follow a specific naming convention), extracts the index and name of each class, and sorts them in the correct order for use in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model before and after fine-tuning\n",
    "# Function to visualize images and their predictions\n",
    "def visualize_predictions(model, dataloader, num_images=2):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move images and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = None\n",
    "\n",
    "            # Forward Pass = Get Predictions\n",
    "            outputs = model(inputs)\n",
    "            outputs = None\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'Predicted: {preds[i].item()}')\n",
    "                img = inputs.cpu().data[i]\n",
    "                img = img.permute(1, 2, 0).numpy()  # Convert from Tensor format\n",
    "                img = np.clip(img, 0, 1)  # Clip the values for display\n",
    "                ax.imshow(img)\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return\n",
    "\n",
    "# Function to denormalize the image for visualization\n",
    "def denormalize(image_tensor):\n",
    "    # Means and stds used for normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = image_tensor.permute(1, 2, 0).numpy()  # Convert tensor to HWC\n",
    "    image = std * image + mean  # Denormalize\n",
    "    image = np.clip(image, 0, 1)  # Clip values to be in [0, 1] range\n",
    "    return image\n",
    "\n",
    "\n",
    "def make_map_classes(path):\n",
    "    classes_dir = os.listdir(path)\n",
    "    classes_dir.sort()\n",
    "    classes_dict = {}\n",
    "    for c in classes_dir:\n",
    "        index = int(c[:3])\n",
    "        name = c[4:]\n",
    "        classes_dict[index] = name\n",
    "\n",
    "    return classes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 8**\n",
    "1) This function, <blue>**compare_predictions**</blue>, is designed to visually compare the predictions made by a <green>**pre-trained model**</green> and a <green>**fine-tuned version**</green> of that model, helping to assess the impact of fine-tuning. It takes in both models, a dataloader, the directory path containing class labels, and the number of images to display.\n",
    "2) <blue>**Model Evaluation Mode**</blue>: Both the <green>**pre-trained**</green> and <green>**fine-tuned models**</green> are set to <green>**evaluation mode**</green> using <blue>**.eval()**</blue>. This ensures that no gradients are calculated during the forward pass, making inference faster and memory-efficient.\n",
    "3)  The TODO lines highlight the need to move the <blue>**input images (inputs)**</blue> to the appropriate device (<green>**CPU**</green> or <green>**GPU**</green>), though this is left as an implementation detail.\n",
    "4) The first subplot shows the prediction from the <blue>**pre-trained model**</blue> along with its <green>**corresponding class name (retrieved from**</green> the <blue>**classes_dict dictionary**</blue>). The second subplot shows the prediction from the <blue>**fine-tuned model**</blue> for comparison. This allows easy visual inspection of how fine-tuning has affected the model's ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare predictions of non-finetuned and finetuned models\n",
    "def compare_predictions(pretrained_model, finetuned_model, dataloader, path_dir, num_images=5):\n",
    "    pretrained_model.eval()\n",
    "    finetuned_model.eval()\n",
    "    \n",
    "    images_shown = 0\n",
    "    classes_dict = make_map_classes(path_dir)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move inputs to the appropriate device (CPU or GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            inputs = None\n",
    "            \n",
    "            # Predictions from pre-trained model (without fine-tuning)\n",
    "            # Calculate outputs using forward pass of pretrained model\n",
    "            pre_outputs = pretrained_model(inputs)\n",
    "            pre_outputs = None\n",
    "            _, pre_preds = torch.max(pre_outputs, 1)\n",
    "            \n",
    "            # Predictions from fine-tuned model\n",
    "            # Calculate the outputs using the forward pass of the finetuned model\n",
    "            fin_outputs = finetuned_model(inputs)\n",
    "            fin_outputs = None \n",
    "            _, fin_preds = torch.max(fin_outputs, 1)\n",
    "\n",
    "            for i in range(inputs.size()[0]):\n",
    "                if images_shown == num_images:\n",
    "                    return  # Stop after showing num_images\n",
    "                images_shown += 1\n",
    "                \n",
    "                img = denormalize(inputs.cpu().data[i])\n",
    "\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                \n",
    "                # Show pre-trained model's prediction\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f'Pre-trained Prediction: {pre_preds[i].item()} : {classes_dict[pre_preds[i].item()]}')\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Show fine-tuned model's prediction\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f'Fine-tuned Prediction: {fin_preds[i].item()} : {classes_dict[fin_preds[i].item()+1]}')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blue {\n",
    "  color: skyblue;\n",
    "}\n",
    "\n",
    "red {\n",
    "  color: red;\n",
    "}\n",
    "\n",
    "green {\n",
    "  color: lightgreen;\n",
    "}\n",
    "</style>\n",
    "\n",
    "### **Step - 9**\n",
    "This code snippet is used to <green>**evaluate**</green> and visualize the predictions of both a <blue>**pre-trained**</blue> and a <blue>**fine-tuned model**</blue> on a set of example images. It compares the predictions side by side to assess the <green>**performance improvements**</green> brought by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize predictions\n",
    "print(\"Comparing predictions of pre-trained and fine-tuned models on example images...\")\n",
    "\n",
    "# Ensure both models have the same structure (so, freeze layers for pretrained_model)\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 257)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# Compare predictions using a couple of test images\n",
    "# Fill in the path of the 256_ObjectCategories directory in the variable below.\n",
    "path_to256_ObjectCategories_dir = 'path/to/256_ObjectCategories'  # Update this path as needed\n",
    "path_to256_ObjectCategories_dir = None\n",
    "compare_predictions(pretrained_model, fine_tuned_model, test_loader, path_to256_ObjectCategories_dir ,num_images=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
